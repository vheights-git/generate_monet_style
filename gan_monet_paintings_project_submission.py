# -*- coding: utf-8 -*-
"""GAN_Monet_Paintings_Project_submission.ipynb

Automatically generated by Colab.

# **Problem Description**

Brief description of the problem and data (5 pts)

Briefly describe the challenge problem and generative deep learning models. Describe the size, dimension, structure, etc., of the data.

In the current problem, I have to generate monet style paintings from a given set of image data. A given image consists of content and style. For e.g. an image of mountains may have mountains as a content and type of mountains as a style i.e. rocky, volcano, plateau mountains etc. For a given set of images, I have to generate a new set of images with the same content as original image however in monet style painting. Monet style consists of specific types of brush strokes and color choices.

The approach to generate monet style paintings from images consists of building a generative model capable of learning monet style features. A generative model learns the unique monet style features and allows us to use these features on a different set of new images thereby generating monet style new images. There are multiple deep generative models for e.g. GANs, Style Transfer, pix2pix, cycleGAN, and augmentations of cycleGANs that can be used. In the next section I will make a case for a specific deep generative model suitable for the current problem.

The effectiveness of generated monet style images is calculated using MiFID metric (Memorization-informed Fréchet Inception Distance). MiFID is a metric that evaluates whether a generated image is too close to the training images potentially overfitting. MiFID uses inception network to embed real images and generated images in the feature space and uses cosine similarity to find fraction of generated images that are too close to the training images for a given threshold distance. Lower the MiFID score better the generative model is in relation to generalization.

In this challenge our goal is to build a generative algorithm (model) with a lowest MiFID score. A FID ≤ 10 score means a near-indistinguishable generated images from real images (state-of-the-art models).

The monet paintings dataset contains 300 monet paintings and the image dataset contains 7038 images. A random sample of monet painting and a respective image is 256x256 pixels with three bands (RGB channels).
"""

!pip install tensorflow-addons

!pip install colorama

!pip install exifread

pip install pillow imagehash

# Dependencies

import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
import tensorflow as tf
import tensorflow_addons as tfa
import cv2
import os
import random
import exifread
from PIL import Image
import datetime
import tqdm
from tqdm.notebook import tqdm_notebook
import hashlib
from collections import defaultdict
import shutil
import math


import warnings
warnings.filterwarnings('ignore')


from colorama import Fore, Back, Style
from tensorflow import keras
from tensorflow.keras import layers

y_ = Fore.YELLOW
r_ = Fore.RED
g_ = Fore.GREEN
b_ = Fore.BLUE
m_ = Fore.MAGENTA

AUTOTUNE = tf.data.experimental.AUTOTUNE

print(tf.__version__)

# External data paths
monet_jpg_directory = 'gan-getting-started/monet_jpg/'
photo_jpg_directory = 'gan-getting-started/photo_jpg/'

# Helper function to get image data path
def getImagePaths(path):
    image_names = []
    for dirname, _, filenames in os.walk(path):
        for filename in filenames:
            fullpath = os.path.join(dirname, filename)
            image_names.append(fullpath)
    return image_names

monet_images_path = getImagePaths(monet_jpg_directory)
photo_images_path = getImagePaths(photo_jpg_directory)

print(f"{y_}Number of Monet images: {g_} {len(monet_images_path)}\n")
print(f"{y_}Number of Photo images: {g_} {len(photo_images_path)}\n")

# Helper function to print basic image data info
def print_basic_image_info(image_path):
    try:
        # File system information
        file_stats = os.stat(image_path)
        print("\nBasic Image Information:")
        print(f"File Path: {os.path.abspath(image_path)}")
        print(f"File Size: {file_stats.st_size / 1024:.2f} KB")
        print(f"Last Modified: {datetime.datetime.fromtimestamp(file_stats.st_mtime)}")

        # Image information using Pillow
        with Image.open(image_path) as img:
            print("\nImage Properties:")
            print(f"Format: {img.format}")
            print(f"Mode: {img.mode}")
            print(f"Size (Width x Height): {img.size[0]} x {img.size[1]} pixels")
            print(f"Number of Bands: {len(img.getbands())}")

            # Basic EXIF data if available
            try:
                exif = img._getexif()
                if exif:
                    print("\nBasic EXIF Data:")
                    print(f"Camera Make: {exif.get(271, 'Unknown')}")
                    print(f"Camera Model: {exif.get(272, 'Unknown')}")
                    print(f"Date Taken: {exif.get(36867, 'Unknown')}")
                    print(f"Orientation: {exif.get(274, 'Normal')}")
            except AttributeError:
                print("\nNo EXIF data available")

    except FileNotFoundError:
        print("Error: File not found")
    except Exception as e:
        print(f"Error: {str(e)}")

# Usage example on random monet painting image
print_basic_image_info(monet_images_path[random.randint(0, len(monet_images_path) - 1)])

# Usage example on random image painting image
print_basic_image_info(photo_images_path[random.randint(0, len(photo_images_path) - 1)])

"""# **Exploratory Data Analysis**

Exploratory Data Analysis (EDA) — Inspect, Visualize and Clean the Data (15 pts)

Show a few visualizations like histograms. Describe any data cleaning procedures. Based on your EDA, what is your plan of analysis? - (15 pts)

In the preliminary data analysis, I visualized some RGB histograms of some randomly sampled monet and photo images for visualization. The monet images are different from the photo images remarkably in the style which is noticable in the brush strokes and colors. I ran some statistics telling me that all monet images and photo images are 256x256 pixels with 3 channels (RGB) and no black and white images (outliers) are found in both datasets. However, I found 9 sets of duplicate images in the photo dataset using md5 hash that were removed.


"""

# Helper function to get shape of the image
def getShape(images_paths):
    shape = cv2.imread(images_paths[0]).shape
    for image_path in images_paths:
        image_shape=cv2.imread(image_path).shape
        if (image_shape!=shape):
            return "Different image shape"
        else:
            return "Same image shape " + str(shape)

getShape(monet_images_path)

getShape(photo_images_path)

# Helper function to get random array to get random sample
def random_sample_array(arr, sample_size, replace=False):
    """
    Randomly samples elements from an array into a new array.

    Args:
        arr (numpy.ndarray or list): The input array.
        sample_size (int): The number of elements to sample.
        replace (bool, optional): Whether to allow sampling with replacement. Defaults to False.

    Returns:
        numpy.ndarray: A new array containing the randomly sampled elements.
                       Returns an empty array if sample_size is invalid.
    """
    arr = np.array(arr)
    if sample_size <= 0 or sample_size > arr.size and not replace:
      return np.array([])

    return np.random.choice(arr, size=sample_size, replace=replace)

# Helper function to display random images
def display_multiple_img(images_paths, rows, cols):
    figure, ax = plt.subplots(nrows=rows,ncols=cols,figsize=(8,4) )
    random_images_paths = random_sample_array(images_paths, rows*cols)
    for ind,image_path in enumerate(random_images_paths):
        image=cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        try:
            ax.ravel()[ind].imshow(image)
            ax.ravel()[ind].set_axis_off()
        except:
            continue;
    plt.tight_layout()
    plt.show()

display_multiple_img(monet_images_path, 4, 4)

display_multiple_img(photo_images_path, 4, 4)

# Helper function to create histogram of channels
def hist(image_path):
    plt.figure(figsize=(16, 3))

    img = cv2.imread(image_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    plt.subplot(1, 5, 1)
    plt.imshow(img)
    styling()

    custom_colors = ["#ef233c", "#76da71", "#2667ff","#aea3b0"]
    labels = ['Red Channel', 'Green Channel', 'Blue Channel','Total']

    for i in range(1,4):
        plt.subplot(1, 5, i+1)
        plt.hist(img[:, :, i-1].reshape(-1),bins=64,color=custom_colors[i-1],alpha = 0.6)
        plt.xlabel(labels[i-1],fontsize=10)
        styling()

    plt.subplot(1, 5, 5)
    plt.hist(img.reshape(-1),bins=128,color=custom_colors[3],alpha = 0.6)
    plt.xlabel(labels[3],fontsize=10)
    styling()
    plt.show()

# Helper function to display histograms
def display_hist(images_paths, n_images):
  random_images_paths = random_sample_array(images_paths, n_images)
  for ind,image_path in enumerate(random_images_paths):
    if (ind<n_images):
      hist(image_path)

display_hist(monet_images_path, 3)

display_hist(photo_images_path, 3)

# Helper function image processing
ORIGINAL_SIZE = 256      # original size of the images - do not change

# AUGMENTATION VARIABLES
CROP_SIZE = 64         # final size after crop
RANDOM_ROTATION = 3    # range (0-180), 180 allows all rotation variations, 0=no change
RANDOM_SHIFT = 2        # center crop shift in x and y axes, 0=no change. This cannot be more than (ORIGINAL_SIZE - CROP_SIZE)//2
RANDOM_BRIGHTNESS = 7  # range (0-100), 0=no change
RANDOM_CONTRAST = 5    # range (0-100), 0=no change
RANDOM_90_DEG_TURN = 1  # 0 or 1= random turn to left or right

def readCroppedImage(path, augmentations = True):
    # augmentations parameter is included for counting statistics from images, where we don't want augmentations

    # OpenCV reads the image in bgr format by default
    bgr_img = cv2.imread(path)
    # We flip it to rgb for visualization purposes
    b,g,r = cv2.split(bgr_img)
    rgb_img = cv2.merge([r,g,b])

    if(not augmentations):
        return rgb_img / 255

    #random rotation
    rotation = random.randint(-RANDOM_ROTATION,RANDOM_ROTATION)
    if(RANDOM_90_DEG_TURN == 1):
        rotation += random.randint(-1,1) * 90
    M = cv2.getRotationMatrix2D((48,48),rotation,1)   # the center point is the rotation anchor
    rgb_img = cv2.warpAffine(rgb_img,M,(96,96))

    #random x,y-shift
    x = random.randint(-RANDOM_SHIFT, RANDOM_SHIFT)
    y = random.randint(-RANDOM_SHIFT, RANDOM_SHIFT)

    # crop to center and normalize to 0-1 range
    start_crop = (ORIGINAL_SIZE - CROP_SIZE) // 2
    end_crop = start_crop + CROP_SIZE
    rgb_img = rgb_img[(start_crop + x):(end_crop + x), (start_crop + y):(end_crop + y)] / 255

    # Random flip
    flip_hor = bool(random.getrandbits(1))
    flip_ver = bool(random.getrandbits(1))
    if(flip_hor):
        rgb_img = rgb_img[:, ::-1]
    if(flip_ver):
        rgb_img = rgb_img[::-1, :]

    # Random brightness
    br = random.randint(-RANDOM_BRIGHTNESS, RANDOM_BRIGHTNESS) / 100.
    rgb_img = rgb_img + br

    # Random contrast
    cr = 1.0 + random.randint(-RANDOM_CONTRAST, RANDOM_CONTRAST) / 100.
    rgb_img = rgb_img * cr

    # clip values to 0-1 range
    rgb_img = np.clip(rgb_img, 0, 1.0)

    return rgb_img

# Helper function to find out dark or white images in the data that do not provide any signal
# As we count the statistics, we can check if there are any completely black or white images
dark_th = 10 / 255      # If no pixel reaches this threshold, image is considered too dark
bright_th = 245 / 255   # If no pixel is under this threshold, image is considerd too bright

def print_statistics(images_paths):
  too_dark_idx = []
  too_bright_idx = []

  x_tot = np.zeros(3)
  x2_tot = np.zeros(3)
  counted_ones = 0
  for i, path in tqdm_notebook(enumerate(images_paths), 'computing statistics...(300 in total)'):
      imagearray = readCroppedImage(path, augmentations = False).reshape(-1,3)
      # is this too dark
      if(imagearray.max() < dark_th):
          too_dark_idx.append(idx)
          continue # do not include in statistics
      # is this too bright
      if(imagearray.min() > bright_th):
          too_bright_idx.append(idx)
          continue # do not include in statistics
      x_tot += imagearray.mean(axis=0)
      x2_tot += (imagearray**2).mean(axis=0)
      counted_ones += 1

  channel_avr = x_tot/counted_ones
  channel_std = np.sqrt(x2_tot/counted_ones - channel_avr**2)
  # channel_avr,channel_std
  return too_dark_idx, too_bright_idx, channel_avr, channel_std

too_dark_idx, too_bright_idx, channel_avr, channel_std = print_statistics(monet_images_path)
print('The channel average', channel_avr)
print('The channel standard deviation', channel_std)
print('There was {0} extremely dark image'.format(len(too_dark_idx)))
print('and {0} extremely bright images'.format(len(too_bright_idx)))
print('Dark one:')
print(too_dark_idx)
print('Bright ones:')
print(too_bright_idx)

too_dark_idx, too_bright_idx, channel_avr, channel_std = print_statistics(photo_images_path)
print('The channel average', channel_avr)
print('The channel standard deviation', channel_std)
print('There was {0} extremely dark image'.format(len(too_dark_idx)))
print('and {0} extremely bright images'.format(len(too_bright_idx)))
print('Dark one:')
print(too_dark_idx)
print('Bright ones:')
print(too_bright_idx)

# Set of helper functions to identify duplicate images in the dataset using md5 hashing
def calculate_file_hash(filepath):
    """Calculate MD5 hash of file content"""
    with open(filepath, 'rb') as f:
        return hashlib.md5(f.read()).hexdigest()

def find_exact_duplicates(directory):
    """Find exact duplicate images by file content"""
    hashes = defaultdict(list)

    for root, _, files in os.walk(directory):
        for filename in files:
            if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.gif')):
                path = os.path.join(root, filename)
                file_hash = calculate_file_hash(path)
                hashes[file_hash].append(path)

    return [paths for paths in hashes.values() if len(paths) > 1]

def display_duplicates(duplicate_groups, max_groups=5, images_per_row=4):
    """Visualize duplicate groups using matplotlib"""
    if not duplicate_groups:
        print("No exact duplicates found")
        return

    print(f"Found {len(duplicate_groups)} groups of exact duplicates")

    for group_idx, group in enumerate(duplicate_groups[:max_groups]):
        plt.figure(figsize=(8, 4))
        plt.suptitle(f"Exact Duplicate Group {group_idx+1} ({len(group)} copies)", fontsize=16)

        # Display up to images_per_row images from the group
        for i, img_path in enumerate(group[:images_per_row]):
            try:
                img = Image.open(img_path)
                plt.subplot(1, images_per_row, i+1)
                plt.imshow(img)
                plt.title(os.path.basename(img_path))
                plt.axis('off')
            except Exception as e:
                print(f"Error displaying {img_path}: {str(e)}")

        plt.tight_layout()
        plt.show()

def print_duplicate_info(duplicate_groups):
    """Print information about duplicate groups"""
    if not duplicate_groups:
        print("No duplicates found")
        return

    print("\nDuplicate Groups Information:")
    total_space = 0
    for i, group in enumerate(duplicate_groups, 1):
        group_size = os.path.getsize(group[0])
        wasted_space = group_size * (len(group) - 1)
        total_space += wasted_space

        print(f"\nGroup {i}:")
        print(f"  Number of copies: {len(group)}")
        print(f"  File size: {group_size / 1024:.2f} KB each")
        print(f"  Wasted space: {wasted_space / 1024:.2f} KB")
        print("  Locations:")
        for path in group[:3]:  # Show first 3 paths
            print(f"    - {path}")
        if len(group) > 3:
            print(f"    (...and {len(group)-3} more)")

    print(f"\nTotal wasted space: {total_space / (1024*1024):.2f} MB")

print("Finding exact duplicates by file content...")
monet_duplicate_groups = find_exact_duplicates(monet_jpg_directory)
print(f"Found {len(monet_duplicate_groups)} exact duplicate groups")

print("Finding exact duplicates by file content...")
photo_duplicate_groups = find_exact_duplicates(photo_jpg_directory)
print(f"Found {len(photo_duplicate_groups)} exact duplicate groups")

print_duplicate_info(photo_duplicate_groups)

# Visualize duplicates
display_duplicates(photo_duplicate_groups, max_groups=10)

# Helper functions to visualize and handle duplicate images in the mage dataset
def visualize_duplicates(duplicate_group, group_number):
    """Visualize a single group of duplicates"""
    plt.figure(figsize=(8, 4))
    plt.suptitle(f"Duplicate Group {group_number} ({len(duplicate_group)} copies)", fontsize=16)

    for i, img_path in enumerate(duplicate_group[:4]):  # Show max 4 images
        try:
            img = Image.open(img_path)
            plt.subplot(1, min(4, len(duplicate_group)), i+1)
            plt.imshow(img)
            plt.title(f"{os.path.basename(img_path)}\n{os.path.dirname(img_path)}", fontsize=8)
            plt.axis('off')
        except Exception as e:
            print(f"Error displaying {img_path}: {str(e)}")

    plt.tight_layout()
    plt.show()

def handle_duplicates(duplicate_groups, action='interactive', output_folder='duplicates'):
    """
    Handle duplicate images with multiple options.

    Parameters:
    - duplicate_groups: List of duplicate groups from find_exact_duplicates()
    - action: 'interactive' (ask for each group), 'auto' (keep first),
              'move', 'delete', or 'report'
    - output_folder: Folder to move duplicates to (for 'move' action)
    """
    if not duplicate_groups:
        print("No duplicates found to handle")
        return

    total_duplicates = sum(len(group)-1 for group in duplicate_groups)
    total_space = sum(os.path.getsize(group[0])*(len(group)-1) for group in duplicate_groups)

    print(f"\nFound {len(duplicate_groups)} duplicate groups with {total_duplicates} redundant copies")
    print(f"Potential space savings: {total_space/(1024*1024):.2f} MB\n")

    if action == 'report':
        return

    # Create output folder if needed
    if action == 'move' and not os.path.exists(output_folder):
        os.makedirs(output_folder)

    for group_idx, group in enumerate(duplicate_groups, 1):
        original = group[0]
        duplicates = group[1:]

        print(f"\n{'='*50}")
        print(f"Duplicate Group {group_idx}:")
        print(f"Original: {original}")
        print(f"Duplicates ({len(duplicates)}):")
        for dup in duplicates[:3]:  # Show first 3 duplicates
            print(f"  - {dup}")
        if len(duplicates) > 3:
            print(f"  (...and {len(duplicates)-3} more)")

        # Visualize the duplicates
        visualize_duplicates(group, group_idx)

        if action == 'interactive':
            choice = input("How to handle? (k)eep all, (d)elete dupes, (m)ove dupes, (s)kip: ").lower()
        else:
            choice = action[0]  # First letter of action

        if choice == 'k':  # keep all
            print("Keeping all files")
            continue
        elif choice in ['d', 'delete']:  # delete duplicates
            for dup in duplicates:
                try:
                    os.remove(dup)
                    print(f"Deleted: {dup}")
                except Exception as e:
                    print(f"Error deleting {dup}: {str(e)}")
        elif choice in ['m', 'move']:  # move duplicates
            for dup in duplicates:
                try:
                    dest = os.path.join(output_folder, os.path.basename(dup))
                    # Handle naming conflicts
                    counter = 1
                    while os.path.exists(dest):
                        name, ext = os.path.splitext(os.path.basename(dup))
                        dest = os.path.join(output_folder, f"{name}_{counter}{ext}")
                        counter += 1
                    shutil.move(dup, dest)
                    print(f"Moved to: {dest}")
                except Exception as e:
                    print(f"Error moving {dup}: {str(e)}")
        else:  # skip
            print("Skipping this group")

handle_duplicates(photo_duplicate_groups, action='move', output_folder='duplicates')

monet_images_path = getImagePaths(monet_jpg_directory)
photo_images_path = getImagePaths(photo_jpg_directory)

print(f"{y_}Number of Monet images: {g_} {len(monet_images_path)}\n")
print(f"{y_}Number of Photo images: {g_} {len(photo_images_path)}\n")

"""# **Model Architecture**

Describe your model architecture and reasoning for why you believe that specific architecture would be suitable for this problem. Compare multiple architectures and tune hyperparameters. - (25 pts)

There are multiple architectures that can be used to solve the current challenge. There exists simple GANs, pix2pix, Style Transfer, and cycleGAN. I believe cycleGAN is the best architecture. There are multiple reasons: firstly, pix2pix needs paired data in the context of (input image, target image) whereas cycleGAN works on domain datasets i.e. (domain A, domain B). Secondly, neural transfer style methods such as Style Transfer are optimization methods and do not employ GANs. Style Transfer uses pre-trained neural networks to extract style and content from the style and content image and, a single output image is optimized to match content of input image (CNN high-level features) and style of reference image using Gram matrices.

The cycleGANs learns the domain features from a given domain dataset in our case monet images. This learning helps to customize the image properties of target domain dataset.

# **Approach**

1. Simple GAN (Generative Adversarial Network)
  - I will start with a simple generator and discriminator. Generator learns on the photos dataset and outputs monet style images. The generator weights are updated using the output from the discriminator on the generated monet style image using binary cross entropy loss function. Discriminator learns on the original monet images and monet style images generated by generator. The discriminator weights are updated using the binary cross entropy loss function on the outputs for the original monet style image and generated monet style images. For real monet style images, 1's are used and for generated monet style images 0's are used.
  - Here I implemented a version with validation losses calculations.

2. CycleGAN
  - CycleGAN is an improvement on the simpel GAN where I implemented two generators and two discriminators respectively for monet dataset and photo dataset. More details are mentioned below. Having a separate generator for photo and monet and separate discrimanator for photo and monet is a significant improvement on the simple GAN. Additional loss functions also increases the learning ability of the model. CycleGAN allows for the identity loss and cycle consistency loss terms that aid in learning process.

3. CycleGAN with modified generator (using transformer blocks)
  - In this approach, generators are modified by implementing transformer blocks in the generator architecture that is expected to aid in learning patterns.

4. Future work: CycleGAN with differential augmentation and modified discriminator (2 discriminator heads for monet discriminator)
  -  Here, due to limited monet images data, discriminators overfits quickly. To address this issue, two enhancements are implemented: differential augmentations on the monet images (both original and generated) and modified version of discriminator architecture .i.e 2-headed monet generator is implemented.

Finally, the respective monet generators are evaluated on the MiFID score.

# **Architecture**
A simple GAN consists of one generator (monet_generator) and one discriminator (monet_discriminator).

CycleGAN consists of two generators (monet_generator and photo_generator) and two discriminators (monet_discriminator and photo_discriminator).

monet_generator: Translates images from photos → monet.

photo_generator: Translates images from monet  → photo.

monet_discrimator: Distinguishes real images of monet from fake ones generated by monet_generator.

photo_disriminator: Distinguishes real images of photos from fake ones generated by photo_generator.

The generators and discriminators are implemented by neural network family of models, to be more precise convolutional neural networks in the case of images.

# **Data Augmentation**
Along with standard datasets, I have implemented images augmentation using operations such as flip, crop, translation, and rotation and images further normalized between (-1,1).

# **Generator**

Generator consists of multiple building blocks where each block consisting of convolutional layer, normalization layer, and activation layer. These blocks are categorized into encoder and decoder. Here, I have implemented encoder using 8 such blocks that downsample the images from size 256x256 and 3 channels to size 1x1 and 512 channels creating a bottleneck. Then through decoder implemented using 7 blocks and a final output layer the images of size 256*256 and 3 channels are generated. This architecture is also called Unet architecture, as it resembles the shape U. Some implementations implement residual blocks (Transformer blocks) after encoder and before decoder to discover additional patterns in the images. This has also been implemented as a modified architecture in the later part.

# **Discriminator**

Discriminators are implemented using 4 encoder blocks and intelligent use of padding layers to reduce the dimensions of images from 256x256x3 to 30x30x1.

# **Loss Functions and Optimizer**

Following loss functions are calculated and used to update the neural network parameters using Adam optimizer. Along with standard learning rate parameters for generator and discriminator, I have used a learning rate scheduler that keeps the learning rate constant and then it decreases linearly.

Adversarial Loss: Ensures generated images are realistic (similar to standard GANs). For generator monet_generator and monet_discriminator and analogously for photo_generator and photo_discriminator. For calculating generator losses, binary cross entropy is used. And for target signals for discrimators, 1s for real and 0s for generated images is used.

Cycle-Consistency Loss: Ensures that translating back and forth reconstructs the original image. For photos → monet → photo and similarly for monet → photo → monet (using L1 norm).

Identity Loss (Optional): Encourages generators to preserve colors/textures when input is already from the target domain (using L1 norm).
"""

# Dependencies

import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
import tensorflow_addons as tfa

import os, random, json, PIL, shutil, re, imageio, glob
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns
from sklearn.model_selection import train_test_split

from tensorflow.keras import Model, losses, optimizers
from tensorflow.keras.callbacks import Callback, ModelCheckpoint, ReduceLROnPlateau, EarlyStopping, TensorBoard
from datetime import datetime
from tensorflow.keras.optimizers.schedules import LearningRateSchedule

# Check if CPU is available
cpu_devices = tf.config.list_physical_devices('CPU')
if cpu_devices:
    print("CPU is available")
else:
    print("CPU is not available")

# Check if GPU is available
gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
    print("GPU is available")
    for gpu in gpu_devices:
        print(f"GPU device: {gpu}")
else:
    print("GPU is not available")

print("Num GPUs Available: ", len(tf.config.experimental.list_physical_devices('GPU')))

import warnings
warnings.filterwarnings('ignore')

AUTOTUNE = tf.data.experimental.AUTOTUNE
AUTO = tf.data.experimental.AUTOTUNE

print(tf.__version__)

# Helper function to access data

EXT_PATH = 'gan-getting-started'

MONET_FILENAMES = tf.io.gfile.glob(str(EXT_PATH + '/monet_tfrec/*.tfrec'))

PHOTO_FILENAMES = tf.io.gfile.glob(str(EXT_PATH + '/photo_tfrec/*.tfrec'))

MONET_FILENAMES_JPG = tf.io.gfile.glob(str(EXT_PATH + '/monet_jpg/*.jpg'))

PHOTO_FILENAMES_JPG = tf.io.gfile.glob(str(EXT_PATH + '/photo_jpg/*.jpg'))

def count_data_items(filenames):
    n = [int(re.compile(r"-([0-9]*)\.").search(filename).group(1)) for filename in filenames]
    return np.sum(n)

n_monet_samples = count_data_items(MONET_FILENAMES)
n_photo_samples = count_data_items(PHOTO_FILENAMES)

print(f'Monet TFRecord files: {len(MONET_FILENAMES)}')
print(f'Monet image files: {n_monet_samples}')
print(f'Photo TFRecord files: {len(PHOTO_FILENAMES)}')
print(f'Photo image files: {n_photo_samples}')

# Auxiliary functions to visualize data

IMAGE_SIZE = [256, 256]

def decode_image(image):
    image = tf.image.decode_jpeg(image, channels=3)
    image = (tf.cast(image, tf.float32) / 127.5) - 1
    image = tf.reshape(image, [*IMAGE_SIZE, 3])
    return image

def read_tfrecord(example):
    tfrecord_format = {
        "image_name": tf.io.FixedLenFeature([], tf.string),
        "image": tf.io.FixedLenFeature([], tf.string),
        "target": tf.io.FixedLenFeature([], tf.string)
    }
    example = tf.io.parse_single_example(example, tfrecord_format)
    image = decode_image(example['image'])
    return image

def load_dataset(filenames, labeled=True, ordered=False):
    dataset = tf.data.TFRecordDataset(filenames)
    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTOTUNE)
    return dataset

monet_ds = load_dataset(MONET_FILENAMES, labeled=True).batch(1)
photo_ds = load_dataset(PHOTO_FILENAMES, labeled=True).batch(1)

example_monet = next(iter(monet_ds))
example_photo = next(iter(photo_ds))

plt.subplot(121)
plt.title('Photo')
plt.imshow(example_photo[0] * 0.5 + 0.5)

plt.subplot(122)
plt.title('Monet')
plt.imshow(example_monet[0] * 0.5 + 0.5)

# Essential constants

OUTPUT_CHANNELS = 3
HEIGHT = 256
WIDTH = 256
HEIGHT_RESIZE = 128
WIDTH_RESIZE = 128
CHANNELS = 3
BATCH_SIZE = 16
EPOCHS = 120
TRANSFORMER_BLOCKS = 6
GENERATOR_LR = 2e-4
DISCRIMINATOR_LR = 2e-4

# Helper functions for Data Augmentation, processing and loading for training

def data_augment(image):

    p_rotate = tf.random.uniform([], 0, 1.0, dtype=tf.float32)
    p_spatial = tf.random.uniform([], 0, 1.0, dtype=tf.float32)
    p_crop = tf.random.uniform([], 0, 1.0, dtype=tf.float32)

    # Ensure image is float32 before applying augmentations
    image = tf.cast(image, tf.float32)

    if p_crop > .5:
        image = tf.image.resize(image, [286, 286])
        image = tf.image.random_crop(image, size=[256, 256, 3])
        if p_crop > .9:
            image = tf.image.resize(image, [300, 300])
            image = tf.image.random_crop(image, size=[256, 256, 3])

    if p_rotate > .9:
        image = tf.image.rot90(image, k=3) # rotate 270º
    elif p_rotate > .7:
        image = tf.image.rot90(image, k=2) # rotate 180º
    elif p_rotate > .5:
        image = tf.image.rot90(image, k=1) # rotate 90º

    if p_spatial > .6:
        image = tf.image.random_flip_left_right(image)
        image = tf.image.random_flip_up_down(image)
        if p_spatial > .9:
            image = tf.image.transpose(image)

    return image

def load_dataset(filenames):
    dataset = tf.data.TFRecordDataset(filenames)
    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)
    return dataset

def get_gan_dataset(monet_files, photo_files, augment=None, repeat=True, shuffle=True, batch_size=1):

    monet_ds = load_dataset(monet_files)
    photo_ds = load_dataset(photo_files)

    if augment:
        monet_ds = monet_ds.map(augment, num_parallel_calls=AUTO)
        photo_ds = photo_ds.map(augment, num_parallel_calls=AUTO)

    if repeat:
        monet_ds = monet_ds.repeat()
        photo_ds = photo_ds.repeat()
    if shuffle:
        monet_ds = monet_ds.shuffle(2048)
        photo_ds = photo_ds.shuffle(2048)

    monet_ds = monet_ds.batch(batch_size, drop_remainder=True)
    photo_ds = photo_ds.batch(batch_size, drop_remainder=True)
    monet_ds = monet_ds.cache()
    photo_ds = photo_ds.cache()
    monet_ds = monet_ds.prefetch(AUTO)
    photo_ds = photo_ds.prefetch(AUTO)

    gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))

    return gan_ds

# Auxiliary functions for data loading, processing, training, visualizing

def normalize_img(img):
    img = tf.cast(img, dtype=tf.float32)
    # Map values in the range [-1, 1]
    return (img / 127.5) - 1.0

def decode_image(image):
    image = tf.image.decode_jpeg(image, channels=CHANNELS)
    image = tf.reshape(image, [HEIGHT, WIDTH, CHANNELS])
    return image

def read_tfrecord_mini(example):
    tfrecord_format = {
        'image':      tf.io.FixedLenFeature([], tf.string)
    }
    example = tf.io.parse_single_example(example, tfrecord_format)
    image = decode_image(example['image'])
    return image

def load_dataset(filenames):
    dataset = tf.data.TFRecordDataset(filenames)
    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)
    return dataset

def get_dataset(filenames, augment=None, repeat=True, shuffle=True, batch_size=1):
    dataset = load_dataset(filenames)

    if augment:
        dataset = dataset.map(augment, num_parallel_calls=AUTO)
    dataset = dataset.map(normalize_img, num_parallel_calls=AUTO)
    if repeat:
        dataset = dataset.repeat()
    if shuffle:
        dataset = dataset.shuffle(512)

    dataset = dataset.batch(batch_size)
    dataset = dataset.cache()
    dataset = dataset.prefetch(AUTO)

    return dataset

def load_and_preprocess_image(filename):
    # Read and decode the image
    image = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(image, channels=3)
    image = tf.image.convert_image_dtype(image, tf.float32)  # Normalize to [0,1]
    return image

def get_dataset_jpg_heavy(filenames, augment=None, repeat=True, shuffle=True, batch_size=1):
    """
    Creates a tf.data.Dataset from JPEG image files.

    Args:
        filenames: List of JPEG file paths
        augment: Optional augmentation function
        repeat: Whether to repeat the dataset indefinitely
        shuffle: Whether to shuffle the dataset
        batch_size: Batch size

    Returns:
        tf.data.Dataset
    """

    # Create dataset from filenames
    dataset = tf.data.Dataset.from_tensor_slices(filenames)

    # Load and preprocess images in parallel
    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=AUTO)

    # Apply augmentation if specified
    if augment:
        dataset = dataset.map(augment, num_parallel_calls=AUTO)

    # Standard image normalization (to [-1,1] range)
    dataset = dataset.map(lambda x: (x * 2) - 1, num_parallel_calls=AUTO)

    # Repeat and shuffle
    if repeat:
        dataset = dataset.repeat()
    if shuffle:
        dataset = dataset.shuffle(512)

    # Batch and optimize
    dataset = dataset.batch(batch_size)
    dataset = dataset.cache()
    dataset = dataset.prefetch(AUTO)

    return dataset

def get_dataset_jpg_light(filenames, augment=None, repeat=True, shuffle=True, batch_size=1):
    """
    Creates an optimized tf.data.Dataset from JPEG files with minimal disk usage.
    """
    # 1. Use AUTOTUNE for dynamic parallelism
    AUTO = tf.data.AUTOTUNE

    # 2. Create dataset from filenames (lightweight operation)
    dataset = tf.data.Dataset.from_tensor_slices(filenames)

    # 3. Shuffle filenames first (reduces disk seeks)
    if shuffle:
        dataset = dataset.shuffle(len(filenames), reshuffle_each_iteration=True)

    # 4. Parallelize image loading and preprocessing
    dataset = dataset.map(
        load_and_preprocess_image,
        num_parallel_calls=AUTO  # Max parallel threads
    )

    # 5. Cache AFTER loading/preprocessing (avoid reprocessing)
    dataset = dataset.cache()  # Cache in RAM or a fast SSD

    # 6. Apply augmentation (if any) after caching
    if augment:
        dataset = dataset.map(
            augment,
            num_parallel_calls=AUTO  # Parallelize augmentation
        )

    # 7. Normalize (lightweight operation)
    dataset = dataset.map(
        lambda x: (x * 2) - 1,  # Normalize to [-1, 1]
        num_parallel_calls=AUTO
    )

    # 8. Repeat indefinitely (if needed)
    if repeat:
        dataset = dataset.repeat()

    # 9. Batch and prefetch
    dataset = dataset.batch(batch_size)
    dataset = dataset.prefetch(AUTO)  # Overlap CPU/GPU work

    return dataset

def get_dataset_jpg_std(filenames):
    """
    Creates a tf.data.Dataset from JPEG image files with standard preprocessing.
    """
    # 1. Use AUTOTUNE for dynamic parallelism
    AUTO = tf.data.AUTOTUNE

    # 2. Create dataset from filenames (lightweight operation)
    dataset = tf.data.Dataset.from_tensor_slices(filenames)

    # 4. Parallelize image loading and preprocessing
    dataset = dataset.map(
        load_and_preprocess_image,
        num_parallel_calls=AUTO  # Max parallel threads
    )

    # 7. Normalize (lightweight operation)
    dataset = dataset.map(
        lambda x: (x * 2) - 1,  # Normalize to [-1, 1]
        num_parallel_calls=AUTO
    )

    return dataset


def display_samples(ds, n_samples):
    ds_iter = iter(ds)
    for n_sample in range(n_samples):
        example_sample = next(ds_iter)
        plt.subplot(121)
        plt.imshow(example_sample[0] * 0.5 + 0.5)
        plt.axis('off')
        plt.show()

def show_img(img_tensor, nrow, title=""):
    img_tensor = img_tensor.detach().cpu() * 0.5 + 0.5
    img_grid = make_grid(img_tensor, nrow=nrow).permute(1, 2, 0)
    plt.figure(figsize=(10, 7))
    plt.imshow(img_grid)
    plt.axis("off")
    plt.title(title)
    plt.show()

def display_generated_samples(ds, model, n_samples):
    ds_iter = iter(ds)
    for n_sample in range(n_samples):
        example_sample = next(ds_iter)
        generated_sample = model.predict(example_sample)

        f = plt.figure(figsize=(12, 12))

        plt.subplot(121)
        plt.title('Input image')
        plt.imshow(example_sample[0] * 0.5 + 0.5)
        plt.axis('off')

        plt.subplot(122)
        plt.title('Generated image')
        plt.imshow(generated_sample[0] * 0.5 + 0.5)
        plt.axis('off')
        plt.show()

def evaluate_cycle(ds, generator_a, generator_b, n_samples=1):
    fig, axes = plt.subplots(n_samples, 3, figsize=(22, (n_samples*6)))
    axes = axes.flatten()

    ds_iter = iter(ds)
    for n_sample in range(n_samples):
        idx = n_sample*3
        example_sample = next(ds_iter)
        generated_a_sample = generator_a.predict(example_sample)
        generated_b_sample = generator_b.predict(generated_a_sample)

        axes[idx].set_title('Input image', fontsize=18)
        axes[idx].imshow(example_sample[0] * 0.5 + 0.5)
        axes[idx].axis('off')

        axes[idx+1].set_title('Generated image', fontsize=18)
        axes[idx+1].imshow(generated_a_sample[0] * 0.5 + 0.5)
        axes[idx+1].axis('off')

        axes[idx+2].set_title('Cycled image', fontsize=18)
        axes[idx+2].imshow(generated_b_sample[0] * 0.5 + 0.5)
        axes[idx+2].axis('off')

    plt.show()

def create_gif(images_path, gif_path):
    images = []
    filenames = glob.glob(images_path)
    filenames.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))
    for epoch, filename in enumerate(filenames):
        img = PIL.ImageDraw.Image.open(filename)
        ImageDraw.Draw(img).text((0, 0),  # Coordinates
                                 f'Epoch {epoch+1}')
        images.append(img)
    imageio.mimsave(gif_path, images, fps=2) # Save gif

def predict_and_save(input_ds, generator_model, output_path):
    i = 1
    for img in input_ds:
        prediction = generator_model(img, training=False)[0].numpy() # make predition
        prediction = (prediction * 127.5 + 127.5).astype(np.uint8)   # re-scale
        im = PIL.Image.fromarray(prediction)
        im.save(f'{output_path}{str(i)}.jpg')
        i += 1

# Defining downsample and upsample methods used in building generator and discriminator blocks.

initializer = tf.random_normal_initializer(0., 0.02)
gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)

def down_sample_block(filters, size, use_stride = 2, use_padding='same', bias=False, use_norm='instancenorm', use_activation='leaky_relu', name='block_x'):

    result = keras.Sequential()
    result.add(layers.Conv2D(filters, size, strides=use_stride, padding=use_padding,
                             kernel_initializer=initializer, use_bias= bias, name=f'encoder_{name}'))

    if use_norm == 'instancenorm':
      result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))
    elif use_norm == 'batchnorm':
      result.add(layers.BatchNormalization())
    elif use_norm == 'none':
      pass

    if use_activation ==  'relu':
      result.add(layers.ReLU())
    elif use_activation ==  'leaky_relu':
      result.add(layers.LeakyReLU(0.2))
    elif use_activation ==  'gelu':
      result.add(layers.Activation(tf.nn.gelu))

    return result


def up_sample_block(filters, size, use_stride = 2, use_padding = 'same', bias = False, use_norm = 'instancenorm', apply_dropout=False, use_activation = 'leaky_relu', name='block_x'):

    result = keras.Sequential()
    result.add(layers.Conv2DTranspose(filters, size, strides=use_stride,
                                      padding= use_padding,
                                      kernel_initializer=initializer,
                                      use_bias= bias, name=f'decoder_{name}'))

    if use_norm == 'instancenorm':
                 result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))
    elif use_norm == 'batchnorm':
                 result.add(layers.BatchNormalization())
    elif use_norm == 'none':
                 pass

    if apply_dropout:
                 result.add(layers.Dropout(0.5))

    if use_activation ==  'relu':
                 result.add(layers.ReLU())
    elif use_activation ==  'leaky_relu':
                 result.add(layers.LeakyReLU(0.2))
    elif use_activation == 'gelu':
                 result.add(layers.Activation(tf.nn.gelu))

    return result

# Implementing generator architecture Unet architecture with bottleneck
def generator_module(height=HEIGHT, width=WIDTH, channels=CHANNELS):

    # Input layer
    inputs = layers.Input(shape=[height, width, channels], name='input_image')

    # Downsampling blocks
    # bs = batch size
    down_sample_block_1 = down_sample_block(64, 4, use_norm = 'none', name='down_sample_block_1') # (bs, 128, 128, 64)
    down_sample_block_2 = down_sample_block(128, 4, name='down_sample_block_2') # (bs, 64, 64, 128)
    down_sample_block_3 = down_sample_block(256, 4, name='down_sample_block_3') # (bs, 32, 32, 256)
    down_sample_block_4 = down_sample_block(512, 4, name='down_sample_block_4') # (bs, 16, 16, 512)
    down_sample_block_5 = down_sample_block(512, 4, name='down_sample_block_5') # (bs, 8, 8, 512)
    down_sample_block_6 = down_sample_block(512, 4, name='down_sample_block_6') # (bs, 4, 4, 512)
    down_sample_block_7 = down_sample_block(512, 4, name='down_sample_block_7') # (bs, 2, 2, 512)
    down_sample_block_8 = down_sample_block(512, 4, name='down_sample_block_8') # (bs, 1, 1, 512)

    # Upsampling blocks
    up_sample_block_1 = up_sample_block(512, 4, apply_dropout=True, name='up_sample_block_1') # (bs, 2, 2, 1024)
    up_sample_block_2 = up_sample_block(512, 4, apply_dropout=True, name='up_sample_block_2') # (bs, 4, 4, 1024)
    up_sample_block_3 = up_sample_block(512, 4, apply_dropout=True, name='up_sample_block_3') # (bs, 8, 8, 1024)
    up_sample_block_4 = up_sample_block(512, 4, name='up_sample_block_4') # (bs, 16, 16, 1024)
    up_sample_block_5 = up_sample_block(256, 4, name='up_sample_block_5') # (bs, 32, 32, 512)
    up_sample_block_6 = up_sample_block(128, 4, name='up_sample_block_6') # (bs, 64, 64, 256)
    up_sample_block_7 = up_sample_block(64, 4, name='up_sample_block_7') # (bs, 128, 128, 128)

    # Output layer
    output_layer = layers.Conv2DTranspose(channels, 4, strides=2, padding='same', kernel_initializer=initializer, activation='tanh')

    down_sample_stack = [
        down_sample_block_1,
        down_sample_block_2,
        down_sample_block_3,
        down_sample_block_4,
        down_sample_block_5,
        down_sample_block_6,
        down_sample_block_7,
        down_sample_block_8
    ]

    up_sample_stack = [
        up_sample_block_1,
        up_sample_block_2,
        up_sample_block_3,
        up_sample_block_4,
        up_sample_block_5,
        up_sample_block_6,
        up_sample_block_7
    ]

    x = inputs

    # Downsampling through the model
    skips = []
    for down in down_sample_stack:
        x = down(x)
        skips.append(x)

    skips = reversed(skips[:-1])

    # Upsampling and establishing the skip connections
    for up, skip_connection in zip(up_sample_stack, skips):
        x = up(x)
        x = layers.Concatenate()([x, skip_connection])

    x = output_layer(x)

    return keras.Model(inputs=inputs, outputs=x)

# Implementing discriminator architecture
def discriminator_module(height=HEIGHT, width=WIDTH, channels=CHANNELS):

    inputs = layers.Input(shape=[height, width, channels], name='input_image')

    down_sample_block_1 = down_sample_block(64, 4, use_norm = 'none', name='down_sample_block_1') # (bs, 128, 128, 64)
    down_sample_block_2 = down_sample_block(128, 4, name='down_sample_block_2') # (bs, 64, 64, 128)
    down_sample_block_3 = down_sample_block(256, 4, name='down_sample_block_3') # (bs, 32, 32, 256)

    zero_pad_1 = layers.ZeroPadding2D() # (bs, 34, 34, 256)

    down_sample_block_4 = down_sample_block(512, 4, use_stride = 1, use_norm = 'instancenorm', use_activation = 'leaky_relu', name='down_sample_block_4') # (bs, 31, 31, 512)

    zero_pad_2 = layers.ZeroPadding2D() # (bs, 33, 33, 512)

    output_layer = layers.Conv2D(1, 4, strides=1,
                                 kernel_initializer=initializer, name='output_layer') # (bs, 30, 30, 1)

    stack = [ down_sample_block_1,
              down_sample_block_2,
              down_sample_block_3,
              zero_pad_1,
              down_sample_block_4,
              zero_pad_2]

    x = inputs

    # Forward pass through the model blocks
    for block in stack:
        x = block(x)

    outputs = output_layer(x)

    return tf.keras.Model(inputs=inputs, outputs=outputs)

class Gan_Validation(keras.Model):
    def __init__(self, generator, discriminator):
        super(Gan_Validation, self).__init__()
        self.generator = generator
        self.discriminator = discriminator
        self.gen_loss_tracker = keras.metrics.Mean(name="gen_loss")
        self.disc_loss_tracker = keras.metrics.Mean(name="disc_loss")
        self.val_gen_loss_tracker = keras.metrics.Mean(name="val_gen_loss")
        self.val_disc_loss_tracker = keras.metrics.Mean(name="val_disc_loss")

    def compile(self, gen_optimizer, disc_optimizer, gen_loss_fn, disc_loss_fn):
        super(Gan_Validation, self).compile()
        self.gen_optimizer = gen_optimizer
        self.disc_optimizer = disc_optimizer
        self.gen_loss_fn = gen_loss_fn
        self.disc_loss_fn = disc_loss_fn

    @property
    def metrics(self):
        return [
            self.gen_loss_tracker,
            self.disc_loss_tracker,
            self.val_gen_loss_tracker,
            self.val_disc_loss_tracker
        ]

    def train_step(self, batch_data):
        real_monet, real_photo = batch_data

        # Train discriminator first
        with tf.GradientTape() as disc_tape:
            fake_monet = self.generator(real_photo, training=True)

            real_output = self.discriminator(real_monet, training=True)
            fake_output = self.discriminator(fake_monet, training=True)

            disc_loss = self.disc_loss_fn(real_output, fake_output)

        disc_gradients = disc_tape.gradient(
            disc_loss, self.discriminator.trainable_variables)
        self.disc_optimizer.apply_gradients(
            zip(disc_gradients, self.discriminator.trainable_variables))

        # Then train generator
        with tf.GradientTape() as gen_tape:
            fake_monet = self.generator(real_photo, training=True)
            fake_output = self.discriminator(fake_monet, training=True)

            gen_loss = self.gen_loss_fn(fake_output)

        gen_gradients = gen_tape.gradient(
            gen_loss, self.generator.trainable_variables)
        self.gen_optimizer.apply_gradients(
            zip(gen_gradients, self.generator.trainable_variables))

        # Update metrics
        self.gen_loss_tracker.update_state(gen_loss)
        self.disc_loss_tracker.update_state(disc_loss)

        return {
            "gen_loss": self.gen_loss_tracker.result(),
            "disc_loss": self.disc_loss_tracker.result()
        }

    def test_step(self, batch_data):
        real_monet, real_photo = batch_data

        # Generate fake images without training
        fake_monet = self.generator(real_photo, training=False)

        # Get discriminator outputs
        real_output = self.discriminator(real_monet, training=False)
        fake_output = self.discriminator(fake_monet, training=False)

        # Calculate validation losses
        val_gen_loss = self.gen_loss_fn(fake_output)
        val_disc_loss = self.disc_loss_fn(real_output, fake_output)

        # Update validation metrics
        self.val_gen_loss_tracker.update_state(val_gen_loss)
        self.val_disc_loss_tracker.update_state(val_disc_loss)

        return {
            "val_gen_loss": self.val_gen_loss_tracker.result(),
            "val_disc_loss": self.val_disc_loss_tracker.result()
        }

# Implementing CycleGAN architecture
class CycleGan(keras.Model):
    def __init__(
        self,
        monet_generator,
        photo_generator,
        monet_discriminator,
        photo_discriminator,
        lambda_cycle=10,
    ):
        super(CycleGan, self).__init__()
        self.m_gen = monet_generator
        self.p_gen = photo_generator
        self.m_disc = monet_discriminator
        self.p_disc = photo_discriminator
        self.lambda_cycle = lambda_cycle

    def compile(
        self,
        m_gen_optimizer,
        p_gen_optimizer,
        m_disc_optimizer,
        p_disc_optimizer,
        gen_loss_fn,
        disc_loss_fn,
        cycle_loss_fn,
        identity_loss_fn
    ):
        super(CycleGan, self).compile()
        self.m_gen_optimizer = m_gen_optimizer
        self.p_gen_optimizer = p_gen_optimizer
        self.m_disc_optimizer = m_disc_optimizer
        self.p_disc_optimizer = p_disc_optimizer
        self.gen_loss_fn = gen_loss_fn
        self.disc_loss_fn = disc_loss_fn
        self.cycle_loss_fn = cycle_loss_fn
        self.identity_loss_fn = identity_loss_fn

    def train_step(self, batch_data):
        real_monet, real_photo = batch_data

        with tf.GradientTape(persistent=True) as tape:
            # photo to monet back to photo
            fake_monet = self.m_gen(real_photo, training=True)
            cycled_photo = self.p_gen(fake_monet, training=True)

            # monet to photo back to monet
            fake_photo = self.p_gen(real_monet, training=True)
            cycled_monet = self.m_gen(fake_photo, training=True)

            # generating itself
            same_monet = self.m_gen(real_monet, training=True)
            same_photo = self.p_gen(real_photo, training=True)

            # discriminator used to check, inputing real images
            disc_real_monet = self.m_disc(real_monet, training=True)
            disc_real_photo = self.p_disc(real_photo, training=True)

            # discriminator used to check, inputing fake images
            disc_fake_monet = self.m_disc(fake_monet, training=True)
            disc_fake_photo = self.p_disc(fake_photo, training=True)

            # evaluates generator loss
            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)
            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)

            # evaluates total cycle consistency loss
            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)

            # evaluates total generator loss
            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)
            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)

            # evaluates discriminator loss
            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)
            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)

        # Calculate the gradients for generator and discriminator
        monet_generator_gradients = tape.gradient(total_monet_gen_loss,
                                                  self.m_gen.trainable_variables)
        photo_generator_gradients = tape.gradient(total_photo_gen_loss,
                                                  self.p_gen.trainable_variables)

        monet_discriminator_gradients = tape.gradient(monet_disc_loss,
                                                      self.m_disc.trainable_variables)
        photo_discriminator_gradients = tape.gradient(photo_disc_loss,
                                                      self.p_disc.trainable_variables)

        # Apply the gradients to the optimizer
        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,
                                                 self.m_gen.trainable_variables))

        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,
                                                 self.p_gen.trainable_variables))

        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,
                                                  self.m_disc.trainable_variables))

        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,
                                                  self.p_disc.trainable_variables))

        return {
            "monet_gen_loss": total_monet_gen_loss,
            "photo_gen_loss": total_photo_gen_loss,
            "monet_disc_loss": monet_disc_loss,
            "photo_disc_loss": photo_disc_loss
        }

# Implementing various loss functions
def discriminator_loss(real, generated):
    real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)

    generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)

    total_disc_loss = real_loss + generated_loss

    return total_disc_loss * 0.5

def generator_loss(generated):
    return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)

def calc_cycle_loss(real_image, cycled_image, LAMBDA):
    loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))

    return LAMBDA * loss1

def identity_loss(real_image, same_image, LAMBDA):
    loss = tf.reduce_mean(tf.abs(real_image - same_image))
    return LAMBDA * 0.5 * loss

"""# **GAN Training**"""

# Implementing training and validation datasets for training GAN with validation losses
# Split filenames first
monet_train, monet_val = train_test_split(MONET_FILENAMES_JPG, test_size=0.2, random_state=42)
photo_train, photo_val = train_test_split(PHOTO_FILENAMES_JPG, test_size=0.2, random_state=42)

# Create separate datasets
train_ds = tf.data.Dataset.zip((
    get_dataset_jpg_light(monet_train, augment=data_augment, batch_size=BATCH_SIZE),
    get_dataset_jpg_light(photo_train, augment=data_augment, batch_size=BATCH_SIZE)
))
val_ds = tf.data.Dataset.zip((
    get_dataset_jpg_light(monet_val, augment=False, batch_size=BATCH_SIZE),  # No augmentation for val
    get_dataset_jpg_light(photo_val, augment=False, batch_size=BATCH_SIZE)
))

train_steps_per_epoch = (max(len(monet_train), len(photo_train))//BATCH_SIZE)
val_steps_per_epoch = (max(len(monet_val), len(photo_val))//BATCH_SIZE)

print(f'Number of monet training samples: {len(monet_train)}')
print(f'Number of monet validation samples: {len(monet_val)}')
print(f'Number of photo training samples: {len(photo_train)}')
print(f'Number of photo validation samples: {len(photo_val)}')
print(f'Training steps per epoch: {train_steps_per_epoch}')
print(f'Validation steps per epoch: {val_steps_per_epoch}')

# Initializing generator and discriminator for GAN
generator = generator_module() # transforms photos to Monet paintings
discriminator = discriminator_module() # differentiates real Monet paintings and generated Monet paintings

# Implementing standard learning rate parameters for GAN
generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

# Helper functions used in training GANs and CycleGANs
# Define the checkpoint callback
checkpoint_path = "gan-getting-started/models_GAN/model_checkpoint_epoch_{epoch:02d}"

# Define the checkpoint callback
checkpoint_epoch_save = ModelCheckpoint(
    checkpoint_path,
    save_freq='epoch',
    save_weights_only=False,
    save_format = 'tf',
    verbose=1
)

checkpoint_learning_rate = ReduceLROnPlateau(
            monitor='val_loss',
            mode='min',          # Reduce LR when val_loss stops decreasing
            factor=0.1,  # Reduce learning rate by a factor of 10
            patience=3,  # Wait for 3 epochs without improvement
            min_lr=1e-7, # Minimum learning rate
            verbose=1
)

early_stopping = EarlyStopping(
    monitor='val_loss',  # Monitor validation loss
    mode='min',          # Stop training when val_loss stops decreasing
    patience=3,          # Stop after 5 epochs without improvement
    min_delta=0.001,     # Minimum change to qualify as improvement
    restore_best_weights=False  # Restore the best model weights
)

# Create a log directory for TensorBoard
log = "gan-getting-started/logs/profile/" + datetime.now().strftime("%Y%m%d-%H%M%S")
tensorboard_callback = TensorBoard(log_dir=log, profile_batch='10,20')


class ResourceUsageCallback(Callback):
    def on_epoch_end(self, epoch, logs=None):
        # Log GPU usage
        result = subprocess.run(['rocm-smi'], stdout=subprocess.PIPE)
        print("GPU Usage:")
        print(result.stdout.decode('utf-8'))

        # Log CPU and memory usage
        cpu_percent = psutil.cpu_percent(interval=1)
        memory_info = psutil.virtual_memory()
        print(f"CPU Usage: {cpu_percent}%")
        print(f"Memory Usage: {memory_info.percent}%")

# Add the callback to model.fit
resource_callback = ResourceUsageCallback()


# Flag to track profiler state
profiler_running = False

def start_profiler(logdir):
    global profiler_running
    tf.profiler.experimental.start(logdir)
    profiler_running = True
    print("Profiler started.")

def stop_profiler():
    global profiler_running
    if profiler_running:
        tf.profiler.experimental.stop()
        profiler_running = False
        print("Profiler stopped.")
    else:
        print("Profiler is not running.")

save_dir = "gan-getting-started/models_gan"
os.makedirs(save_dir, exist_ok=True)

class GANCheckpoint(tf.keras.callbacks.Callback):
    def __init__(self, save_dir):
        super().__init__()
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def on_epoch_end(self, epoch, logs=None):
        self.model.generator.save(f"{self.save_dir}/generator_epoch_{epoch}.h5")
        self.model.discriminator.save(f"{self.save_dir}/discriminator_epoch_{epoch}.h5")

class CYCLEGANCheckpoint(tf.keras.callbacks.Callback):
    def __init__(self, save_dir):
        super().__init__()
        self.save_dir = save_dir
        os.makedirs(save_dir, exist_ok=True)

    def on_epoch_end(self, epoch, logs=None):
        self.model.m_gen.save(f"{self.save_dir}/monet_generator_epoch_{epoch}.h5")
        self.model.p_gen.save(f"{self.save_dir}/photo_generator_epoch_{epoch}.h5")
        self.model.m_disc.save(f"{self.save_dir}/monet_discriminator_epoch_{epoch}.h5")
        self.model.p_disc.save(f"{self.save_dir}/photo_discriminator_epoch_{epoch}.h5")

# Create GAN with Validation losses
gan_model_validation = Gan_Validation(generator, discriminator)

gan_model_validation.compile(gen_optimizer=generator_optimizer,
                  disc_optimizer=discriminator_optimizer,
                  gen_loss_fn=generator_loss,
                  disc_loss_fn=discriminator_loss)

# Training GAN with validation losses
# Assuming train_ds and val_ds are already created (e.g., from tf.data.Dataset.zip)

train_gan_model_validation = gan_model_validation.fit(
    train_ds,                    # Training dataset
    validation_data=val_ds,      # Validation dataset
   #epochs=EPOCHS,               # Number of epochs
    epochs=30,               # Number of epochs
    steps_per_epoch=train_steps_per_epoch,    # Batches for training
    batch_size=BATCH_SIZE,       # Batch size (redundant if already batched)
    validation_steps=val_steps_per_epoch,    # Batches for validation
    verbose=1,                   # Verbosity mode
#   callbacks=[GANMonitor()]    # Callbacks for monitoring
    callbacks=[GANCheckpoint(save_dir)]
)
history_0 = train_gan_model_validation.history

# Continuing traning
train_gan_model_validation = gan_model_validation.fit(
    train_ds,                    # Training dataset
    validation_data=val_ds,      # Validation dataset
    epochs=EPOCHS,               # Number of epochs
    initial_epoch=30,               # Number of epochs
    steps_per_epoch=train_steps_per_epoch,    # Batches for training
    batch_size=BATCH_SIZE,       # Batch size (redundant if already batched)
    validation_steps=val_steps_per_epoch,    # Batches for validation
    verbose=1,                   # Verbosity mode
#   callbacks=[GANMonitor()]    # Callbacks for monitoring
)

"""# **CycleGAN Training**"""

# Initializing generators and discriminators for CycleGAN architecture
monet_generator = generator_module() # transforms photos to Monet paintings
photo_generator = generator_module() # transforms Monet paintings to be more like photos

monet_discriminator = discriminator_module() # differentiates real Monet paintings and generated Monet paintings
photo_discriminator = discriminator_module() # differentiates real photos and generated photos

# Standard learning rate parameters for CycleGAN generators and discriminators
monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

# Create CycleGAN
cyclegan_model = CycleGan(monet_generator, photo_generator,
                      monet_discriminator, photo_discriminator)

cyclegan_model.compile(m_gen_optimizer=monet_generator_optimizer,
                  p_gen_optimizer=photo_generator_optimizer,
                  m_disc_optimizer=monet_discriminator_optimizer,
                  p_disc_optimizer=photo_discriminator_optimizer,
                  gen_loss_fn=generator_loss,
                  disc_loss_fn=discriminator_loss,
                  cycle_loss_fn=calc_cycle_loss,
                  identity_loss_fn=identity_loss)

monet_ds = get_dataset_jpg_light(MONET_FILENAMES_JPG, augment=data_augment, batch_size=BATCH_SIZE)
photo_ds = get_dataset_jpg_light(PHOTO_FILENAMES_JPG, augment=data_augment, batch_size=BATCH_SIZE)
gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))

photo_ds_eval = get_dataset_jpg_std(PHOTO_FILENAMES_JPG).take(1024).batch(32).prefetch(32)
monet_ds_eval = get_dataset_jpg_std(MONET_FILENAMES_JPG).take(1024).batch(32).prefetch(32)

steps_per_epoch = (max(n_monet_samples, n_photo_samples)//BATCH_SIZE)

# Training CycleGAN
save_dir_cyclegan = "gan-getting-started/models_cyclegan"
os.makedirs(save_dir_cyclegan, exist_ok=True)

train_cyclegan_model = cyclegan_model.fit(gan_ds,
                        epochs=50,
                        callbacks=[CYCLEGANCheckpoint(save_dir_cyclegan)],
                        steps_per_epoch=steps_per_epoch,
                        verbose=1)
history_2 = train_cyclegan_model.history

# Continuing training for cycleGAN from saved weights
# Initializing generators and discriminators for CycleGAN architecture
monet_generator = generator_module() # transforms photos to Monet paintings
photo_generator = generator_module() # transforms Monet paintings to be more like photos

monet_discriminator = discriminator_module() # differentiates real Monet paintings and generated Monet paintings
photo_discriminator = discriminator_module() # differentiates real photos and generated photos

# Standard learning rate parameters for CycleGAN generators and discriminators
monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

# Create CycleGAN
cyclegan_model = CycleGan(monet_generator, photo_generator,
                      monet_discriminator, photo_discriminator)

cyclegan_model.m_gen = tf.keras.models.load_model("gan-getting-started/models_cyclegan/monet_generator_epoch_11.h5")
cyclegan_model.p_gen = tf.keras.models.load_model("gan-getting-started/models_cyclegan/photo_generator_epoch_11.h5")
cyclegan_model.m_disc = tf.keras.models.load_model("gan-getting-started/models_cyclegan/monet_discriminator_epoch_11.h5")
cyclegan_model.p_disc = tf.keras.models.load_model("gan-getting-started/models_cyclegan/photo_discriminator_epoch_11.h5")


cyclegan_model.compile(m_gen_optimizer=monet_generator_optimizer,
                  p_gen_optimizer=photo_generator_optimizer,
                  m_disc_optimizer=monet_discriminator_optimizer,
                  p_disc_optimizer=photo_discriminator_optimizer,
                  gen_loss_fn=generator_loss,
                  disc_loss_fn=discriminator_loss,
                  cycle_loss_fn=calc_cycle_loss,
                  identity_loss_fn=identity_loss)

monet_ds = get_dataset_jpg_light(MONET_FILENAMES_JPG, augment=data_augment, batch_size=BATCH_SIZE)
photo_ds = get_dataset_jpg_light(PHOTO_FILENAMES_JPG, augment=data_augment, batch_size=BATCH_SIZE)
gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))

photo_ds_eval = get_dataset_jpg_std(PHOTO_FILENAMES_JPG).take(1024).batch(32).prefetch(32)
monet_ds_eval = get_dataset_jpg_std(MONET_FILENAMES_JPG).take(1024).batch(32).prefetch(32)

steps_per_epoch = (max(n_monet_samples, n_photo_samples)//BATCH_SIZE)

# Training CycleGAN
save_dir_cyclegan = "gan-getting-started/models_cyclegan"
os.makedirs(save_dir_cyclegan, exist_ok=True)

train_cyclegan_model = cyclegan_model.fit(gan_ds,
                        epochs=50,
                        initial_epoch = 12,
                        callbacks=[CYCLEGANCheckpoint(save_dir_cyclegan)],
                        steps_per_epoch=steps_per_epoch,
                        verbose=1)
history_2 = train_cyclegan_model.history

# Continuing training for cycleGAN from saved weights
# Initializing generators and discriminators for CycleGAN architecture
monet_generator = generator_module() # transforms photos to Monet paintings
photo_generator = generator_module() # transforms Monet paintings to be more like photos

monet_discriminator = discriminator_module() # differentiates real Monet paintings and generated Monet paintings
photo_discriminator = discriminator_module() # differentiates real photos and generated photos

# Standard learning rate parameters for CycleGAN generators and discriminators
monet_generator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)
photo_generator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)

monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)
photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)

# Create CycleGAN
cyclegan_model = CycleGan(monet_generator, photo_generator,
                      monet_discriminator, photo_discriminator)

cyclegan_model.m_gen = tf.keras.models.load_model("gan-getting-started/models_cyclegan/monet_generator_epoch_23.h5")
cyclegan_model.p_gen = tf.keras.models.load_model("gan-getting-started/models_cyclegan/photo_generator_epoch_23.h5")
cyclegan_model.m_disc = tf.keras.models.load_model("gan-getting-started/models_cyclegan/monet_discriminator_epoch_23.h5")
cyclegan_model.p_disc = tf.keras.models.load_model("gan-getting-started/models_cyclegan/photo_discriminator_epoch_23.h5")


cyclegan_model.compile(m_gen_optimizer=monet_generator_optimizer,
                  p_gen_optimizer=photo_generator_optimizer,
                  m_disc_optimizer=monet_discriminator_optimizer,
                  p_disc_optimizer=photo_discriminator_optimizer,
                  gen_loss_fn=generator_loss,
                  disc_loss_fn=discriminator_loss,
                  cycle_loss_fn=calc_cycle_loss,
                  identity_loss_fn=identity_loss)

monet_ds = get_dataset_jpg_light(MONET_FILENAMES_JPG, augment=data_augment, batch_size=BATCH_SIZE)
photo_ds = get_dataset_jpg_light(PHOTO_FILENAMES_JPG, augment=data_augment, batch_size=BATCH_SIZE)
gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))

photo_ds_eval = get_dataset_jpg_std(PHOTO_FILENAMES_JPG).take(1024).batch(32).prefetch(32)
monet_ds_eval = get_dataset_jpg_std(MONET_FILENAMES_JPG).take(1024).batch(32).prefetch(32)

steps_per_epoch = (max(n_monet_samples, n_photo_samples)//BATCH_SIZE)

# Training CycleGAN
save_dir_cyclegan = "gan-getting-started/models_cyclegan"
os.makedirs(save_dir_cyclegan, exist_ok=True)

train_cyclegan_model = cyclegan_model.fit(gan_ds,
                        epochs=50,
                        initial_epoch = 24,
                        callbacks=[CYCLEGANCheckpoint(save_dir_cyclegan)],
                        steps_per_epoch=steps_per_epoch,
                        verbose=1)
history_2 = train_cyclegan_model.history

# Loading cycleGAN from saved weights for inference
# Initializing generators and discriminators for CycleGAN architecture
monet_generator = generator_module() # transforms photos to Monet paintings
photo_generator = generator_module() # transforms Monet paintings to be more like photos

monet_discriminator = discriminator_module() # differentiates real Monet paintings and generated Monet paintings
photo_discriminator = discriminator_module() # differentiates real photos and generated photos

# Standard learning rate parameters for CycleGAN generators and discriminators
monet_generator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)
photo_generator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)

monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)
photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)

# Create CycleGAN
cyclegan_model = CycleGan(monet_generator, photo_generator,
                      monet_discriminator, photo_discriminator)

cyclegan_model.m_gen = tf.keras.models.load_model("gan-getting-started/models_cyclegan/monet_generator_epoch_35.h5")
cyclegan_model.p_gen = tf.keras.models.load_model("gan-getting-started/models_cyclegan/photo_generator_epoch_35.h5")
cyclegan_model.m_disc = tf.keras.models.load_model("gan-getting-started/models_cyclegan/monet_discriminator_epoch_35.h5")
cyclegan_model.p_disc = tf.keras.models.load_model("gan-getting-started/models_cyclegan/photo_discriminator_epoch_35.h5")


cyclegan_model.compile(m_gen_optimizer=monet_generator_optimizer,
                  p_gen_optimizer=photo_generator_optimizer,
                  m_disc_optimizer=monet_discriminator_optimizer,
                  p_disc_optimizer=photo_discriminator_optimizer,
                  gen_loss_fn=generator_loss,
                  disc_loss_fn=discriminator_loss,
                  cycle_loss_fn=calc_cycle_loss,
                  identity_loss_fn=identity_loss)

"""# **Modified Architecture : Generator with Transformer Blocks**

In the modified architecture, I am modifying the generator. I am going to use 3 downsample blocks -> 6 (transformer blocks) -> 3 upsample blocks.
"""

# Function that implements transformer block
def transformer_block(filters, size, use_stride = 2, use_padding='same', bias=False, use_norm='instancenorm', use_activation='leaky_relu', name='block_x'):

    result = keras.Sequential()
    result.add(layers.Conv2D(filters, size, strides=use_stride, padding=use_padding,
                             kernel_initializer=initializer, use_bias= bias, name=f'transformer_{name}'))

    if use_norm == 'instancenorm':
      result.add(tfa.layers.InstanceNormalization(gamma_initializer=gamma_init))
    elif use_norm == 'batchnorm':
      result.add(layers.BatchNormalization())
    elif use_norm == 'none':
      pass

    if use_activation ==  'relu':
      result.add(layers.ReLU())
    elif use_activation ==  'leaky_relu':
      result.add(layers.LeakyReLU(0.2))
    elif use_activation ==  'gelu':
      result.add(layers.Activation(tf.nn.gelu))

    return result

# Function implements generator with transformer blocks
def modified_generator_module(height=HEIGHT, width=WIDTH, channels=CHANNELS):

    # Input layer
    inputs = layers.Input(shape=[height, width, channels], name='input_image')

    # Downsampling blocks
    # bs = batch size
    down_sample_block_1 = down_sample_block(64, 4, use_norm = 'none', name='down_sample_block_1') # (bs, 128, 128, 64)
    down_sample_block_2 = down_sample_block(128, 4, name='down_sample_block_2') # (bs, 64, 64, 128)
    down_sample_block_3 = down_sample_block(256, 4, name='down_sample_block_3') # (bs, 32, 32, 256)

    # Transformer blocks
    transformer_block_1 = transformer_block(256,3,1, use_activation='relu', name='transformer_block_1') # (bs,32, 32, 256)
    transformer_block_2 = transformer_block(256,3,1, use_activation='relu', name='transformer_block_2') # (bs,32, 32, 256)
    transformer_block_3 = transformer_block(256,3,1, use_activation='relu', name='transformer_block_3') # (bs,32, 32, 256)
    transformer_block_4 = transformer_block(256,3,1, use_activation='relu', name='transformer_block_4') # (bs,32, 32, 256)
    transformer_block_5 = transformer_block(256,3,1, use_activation='relu', name='transformer_block_5') # (bs,32, 32, 256)
    transformer_block_6 = transformer_block(256,3,1, use_activation='relu', name='transformer_block_6') # (bs,32, 32, 256)

    # Upsampling blocks
    up_sample_block_1 = up_sample_block(256, 4, name='up_sample_block_1') # (bs, 32, 32, 512)
    up_sample_block_2 = up_sample_block(128, 4, name='up_sample_block_2') # (bs, 64, 64, 256)
    up_sample_block_3 = up_sample_block(64, 4, name='up_sample_block_3') # (bs, 128, 128, 128)

    # Output layer
    output_layer = layers.Conv2DTranspose(channels, 4, strides=2, padding='same', kernel_initializer=initializer, activation='tanh')

    down_sample_stack = [
        down_sample_block_1,
        down_sample_block_2,
        down_sample_block_3,
    ]

    transformer_stack = [
        transformer_block_1,
        transformer_block_2,
        transformer_block_3,
        transformer_block_4,
        transformer_block_5,
        transformer_block_6
    ]

    up_sample_stack = [
        up_sample_block_1,
        up_sample_block_2,
        up_sample_block_3,
    ]

    x = inputs

    # Downsampling through the model
    skips = []
    for down in down_sample_stack:
        x = down(x)
        skips.append(x)

    skips = reversed(skips[:-1])

    # Forward pass through the model blocks
    for block in transformer_stack:
        x = block(x)

    # Upsampling and establishing the skip connections
    for up, skip_connection in zip(up_sample_stack, skips):
        x = up(x)
        x = layers.Concatenate()([x, skip_connection])

    x = output_layer(x)

    return keras.Model(inputs=inputs, outputs=x)

"""# **Modified cycleGAN Training**"""

# Initializing CycleGAN with modified generator
modified_monet_generator = modified_generator_module() # transforms photos to Monet paintings
modified_photo_generator = modified_generator_module() # transforms Monet paintings to be more like photos

monet_discriminator = discriminator_module() # differentiates real Monet paintings and generated Monet paintings
photo_discriminator = discriminator_module() # differentiates real photos and generated photos

# Standard learning rate parameters for modified generator CycleGAN architecture
monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)
photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)

# Create GAN
modified_gen_cyclegan_model = CycleGan(modified_monet_generator, modified_photo_generator,
                      monet_discriminator, photo_discriminator)

modified_gen_cyclegan_model.compile(m_gen_optimizer=monet_generator_optimizer,
                  p_gen_optimizer=photo_generator_optimizer,
                  m_disc_optimizer=monet_discriminator_optimizer,
                  p_disc_optimizer=photo_discriminator_optimizer,
                  gen_loss_fn=generator_loss,
                  disc_loss_fn=discriminator_loss,
                  cycle_loss_fn=calc_cycle_loss,
                  identity_loss_fn=identity_loss)

monet_ds = get_dataset_jpg_light(MONET_FILENAMES_JPG, augment=data_augment, batch_size=BATCH_SIZE)
photo_ds = get_dataset_jpg_light(PHOTO_FILENAMES_JPG, augment=data_augment, batch_size=BATCH_SIZE)
gan_ds = tf.data.Dataset.zip((monet_ds, photo_ds))

photo_ds_eval = get_dataset_jpg_std(PHOTO_FILENAMES_JPG).take(1024).batch(32).prefetch(32)
monet_ds_eval = get_dataset_jpg_std(MONET_FILENAMES_JPG).take(1024).batch(32).prefetch(32)

steps_per_epoch = (max(n_monet_samples, n_photo_samples)//BATCH_SIZE)

# Training modified generator CycleGAN model
train_modified_gen_cyclegan_model = modified_gen_cyclegan_model.fit(gan_ds,
                        epochs=EPOCHS,
                        callbacks=[GANMonitor()],
                        steps_per_epoch=total_steps_epoch,
                        verbose=1)
history_3 = train_modified_gen_cyclegan_model.history

# Training CycleGAN
save_dir_cyclegan_modified_gen = "gan-getting-started/models_cyclegan_modified_gen"
os.makedirs(save_dir_cyclegan_modified_gen, exist_ok=True)

train_modified_gen_cyclegan_model = modified_gen_cyclegan_model.fit(gan_ds,
                        epochs=50,
                        callbacks=[CYCLEGANCheckpoint(save_dir_cyclegan_modified_gen)],
                        steps_per_epoch=steps_per_epoch,
                        verbose=1)
history_3 = train_modified_gen_cyclegan_model.history

"""# **Results and Analysis**

Run hyperparameter tuning, try different architectures for comparison, apply techniques to improve training or performance, and discuss what helped. Includes results with tables and figures. There is an analysis of why or why not something worked well, troubleshooting, and a hyperparameter optimization procedure summary. - (35 pts)

In summary, I implemented simple GAN, cycleGAN, and modified version of cycleGAN. I analyzed the results from three perspectives: training loss metrics, MiFID score, and visual quality.

From the perspective of training metrics, cycleGAN training was very stable and smooth. GAN training was unstable throughout its 46 epochs of training. Modified cycleGAN training was stable, though I saw some fluctuations in the end likely to be of mode collapse. I implemented validation losses for GAN training to learn about model generalization. In case of cycleGAN and modified cycleGAN, implementing validation losses would have been complex. I leave it for future work. Here cycleGAN is clearly winner.

From the perspective of MiFID score, the lowest score is achieved by cycleGAN of 10.88. GAN achieved a score of 11.11 whereas modified cycleGAN achieved a score of 12.58. Clearly cycleGAN has the lowest score. Typically, MiFID ≤ 10 means near-indistinguishable from real monet images (state-of-the-art models). MiFID ~ 20-50 means good but noticeable artifacts. Here cycleGAN is the winner.

From the perspective of visual quality of 16 samples, I did not find any noticeable artifacts in the modified cycleGAN generated images. Whereas, GAN generated monet paintings were of low quality compared to cycleGAN and modified cycleGAN generated monet paintings.

If I were to explain these results, it is clear that cycleGAN is much better than vanilla GAN because cycleGAN has two generators, and two discriminators and additional losses such as cycle loss and identity loss that helps in better learning. Modified cycleGAN only differs in the generator architecture. In place of Unet architecture, modified cycleGAN has transformers blocks and less convolutional layers, so it seems that modified cycleGAN learning of monet style features is limited.

One of the concern is that I have only 300 monet paintings to learn from, so with the limited domain dataset discriminator tends to overfit. I leave implementing two headed discrimator and differential augmentation for future work.
"""

import matplotlib.pyplot as plt

"""# **GAN Tranining Loss Visualization**"""

train_gen_loss = [
    1.3741, 1.2560, 1.1702, 1.1019, 1.0504, 1.0376, 1.0055, 1.0201, 1.0023, 1.0075,
    0.9573, 0.9310, 0.9474, 0.9239, 0.9260, 0.9324, 0.9304, 0.9449, 0.9566, 0.9408,
    0.9596, 0.9600, 0.9825, 0.9787, 0.9994, 1.0162, 1.0477, 1.0761, 1.1164, 1.1232,
    0.8741, 0.8728, 0.8758, 0.8758, 0.8781, 0.8897, 0.8859, 0.8808, 0.8919, 0.9008,
    0.9121, 0.9234, 0.9297, 0.9372, 0.9384, 0.9435
]

train_disc_loss = [
    0.7124, 0.7177, 0.7521, 0.7737, 0.7899, 0.7902, 0.7910, 0.7915, 0.7954, 0.7991,
    0.8028, 0.8063, 0.8069, 0.8096, 0.8077, 0.8062, 0.8093, 0.8030, 0.8046, 0.8046,
    0.7972, 0.7960, 0.7954, 0.7901, 0.7850, 0.7792, 0.7719, 0.7670, 0.7583, 0.7436,
    0.8159, 0.8153, 0.8148, 0.8136, 0.8129, 0.8119, 0.8140, 0.8116, 0.8095, 0.8076,
    0.8037, 0.8004, 0.7955, 0.7982, 0.7974, 0.7954
]

val_gen_loss = [
    0.8868, 1.4110, 0.7775, 0.8840, 0.9098, 0.9100, 0.8906, 0.8420, 0.9422, 1.0205,
    0.8975, 0.8752, 1.0122, 0.9375, 0.9137, 0.9145, 0.8321, 0.9602, 0.9199, 0.9209,
    0.9122, 0.9521, 0.8849, 0.8815, 0.9099, 0.9062, 0.9812, 0.9372, 0.9025, 1.1538,
    0.8394, 0.8489, 0.8297, 0.9287, 0.8454, 0.8629, 0.8448, 0.8430, 0.8932, 0.9017,
    0.8316, 0.8876, 0.8067, 0.8771, 0.8820, 0.8901
]

val_disc_loss = [
    0.7489, 0.7069, 0.8344, 0.8033, 0.7852, 0.7393, 0.7831, 0.8089, 0.7990, 0.7746,
    0.8049, 0.7781, 0.8183, 0.8503, 0.8081, 0.8173, 0.8122, 0.7738, 0.8920, 0.8026,
    0.7796, 0.8455, 0.8029, 0.7858, 0.8070, 0.7773, 0.8118, 0.7326, 0.7980, 0.7325,
    0.8162, 0.8017, 0.8227, 0.7950, 0.8246, 0.8240, 0.8122, 0.8212, 0.7802, 0.8200,
    0.8465, 0.7893, 0.8603, 0.8043, 0.8032, 0.8226
]

epochs = range(1, 47)

plt.figure(figsize=(12, 6))
plt.plot(epochs, train_gen_loss, label='Training Generator Loss')
plt.plot(epochs, val_gen_loss, label='Validation Generator Loss')
plt.plot(epochs, train_disc_loss, label='Training Discriminator Loss')
plt.plot(epochs, val_disc_loss, label='Validation Discriminator Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Generator and Discriminator Loss Over Training')
plt.legend()
plt.grid(True)
plt.show()

"""# **CycleGAN Training Loss Visualization**"""

monet_gen_loss = [
    4.5156, 3.5827, 3.1960, 2.9278, 2.7575, 2.6856, 2.6299, 2.5868, 2.5359, 2.4838,
    2.4368, 2.3932, 2.3484, 2.3094, 2.3012, 2.2654, 2.2548, 2.2766, 2.2771, 2.2616,
    2.2478, 2.2654, 2.2869, 2.1809, 2.1991, 2.2205, 2.2367, 2.2441, 2.2561, 2.2660,
    2.2710, 2.2843, 2.2904, 2.2952, 2.2999, 2.3036
]

photo_gen_loss = [
    4.7132, 3.7665, 3.4015, 3.1388, 2.9184, 2.8070, 2.7429, 2.6870, 2.6597, 2.6211,
    2.5777, 2.5425, 2.5111, 2.4574, 2.4422, 2.4117, 2.4111, 2.3766, 2.3900, 2.3701,
    2.3644, 2.3506, 2.3455, 2.3107, 2.2741, 2.2842, 2.2956, 2.3003, 2.3094, 2.3096,
    2.3098, 2.3202, 2.3210, 2.3204, 2.3198, 2.3217
]

monet_disc_loss = [
    0.5405, 0.5479, 0.5712, 0.6000, 0.6039, 0.5995, 0.5987, 0.5995, 0.6044, 0.6115,
    0.6103, 0.6133, 0.6116, 0.6187, 0.6141, 0.6404, 0.6117, 0.6063, 0.6076, 0.6044,
    0.6078, 0.5965, 0.5893, 0.6454, 0.5561, 0.5528, 0.5509, 0.5526, 0.5531, 0.5512,
    0.5516, 0.5503, 0.5505, 0.5498, 0.5502, 0.5489
]

photo_disc_loss = [
    0.4838, 0.5093, 0.5197, 0.5442, 0.5585, 0.5672, 0.5731, 0.5774, 0.5729, 0.5735,
    0.5741, 0.5753, 0.5711, 0.5880, 0.5819, 0.5845, 0.5848, 0.5865, 0.5841, 0.5823,
    0.5851, 0.5784, 0.5812, 0.5789, 0.5412, 0.5435, 0.5445, 0.5464, 0.5477, 0.5495,
    0.5531, 0.5524, 0.5538, 0.5562, 0.5577, 0.5587
]

epochs = range(1, 37)

plt.figure(figsize=(12, 6))
plt.plot(epochs, monet_gen_loss, label='Monet Generator Loss')
plt.plot(epochs, photo_gen_loss, label='Photo Generator Loss')
plt.plot(epochs, monet_disc_loss, label='Monet Discriminator Loss')
plt.plot(epochs, photo_disc_loss, label='Photo Discriminator Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Generator and Discriminator Losses Over Training')
plt.legend()
plt.show()

"""# **Modified Generator CycleGAN Training Loss Visualization**"""

monet_gen_loss = [
    4.4829, 3.5142, 3.0893, 2.8867, 2.8516, 2.8081, 2.6342, 2.5526, 2.5205, 2.5225,
    2.4881, 2.4654, 2.4646, 2.5039, 2.5771, 2.6148, 2.5149, 2.6246, 2.7386, 2.7367,
    2.7352, 2.7448, 2.7933, 2.7982, 2.7497, 2.7769, 2.9157, 2.9735, 2.8871, 2.8952,
    3.0001, 3.0639, 2.9161, 3.0084, 2.7676, 2.9635, 3.0903, 3.1302, 3.1551, 2.8412,
    3.1205, 3.1038, 3.1642, 3.3118, 3.0261, 3.2401, 3.3595, 3.3913, 3.4563, 2.3419
]

photo_gen_loss = [
    4.6349, 3.5908, 3.1406, 2.9381, 2.7998, 2.7842, 2.7636, 2.7408, 2.7105, 2.6925,
    2.6220, 2.6076, 2.6752, 2.6084, 2.6511, 2.6739, 2.6148, 2.8138, 2.8455, 2.7934,
    2.7979, 2.8713, 2.8012, 2.7432, 2.8465, 2.7186, 2.7416, 2.7103, 2.6833, 2.6847,
    2.7596, 2.8049, 2.7533, 2.7534, 2.7175, 2.7944, 2.8923, 2.8350, 2.7188, 2.7402,
    2.7399, 2.7546, 2.7867, 2.7955, 2.7323, 2.7250, 2.7930, 2.7853, 2.7970, 2.6390
]

monet_disc_loss = [
    0.5596, 0.5345, 0.5607, 0.5637, 0.5511, 0.5583, 0.6009, 0.6049, 0.6014, 0.5988,
    0.6020, 0.5995, 0.5970, 0.5856, 0.5713, 0.5716, 0.5781, 0.5728, 0.5524, 0.5408,
    0.5381, 0.5305, 0.5146, 0.5679, 0.5047, 0.4953, 0.4709, 0.4760, 0.4701, 0.4668,
    0.4493, 0.5177, 0.4423, 0.4243, 0.6447, 0.4170, 0.4017, 0.3903, 0.5696, 0.4457,
    0.3730, 0.5055, 0.3547, 0.4570, 0.3837, 0.3390, 0.3353, 0.3271, 0.3306, 0.8600
]

photo_disc_loss = [
    0.5132, 0.5311, 0.5428, 0.5415, 0.5555, 0.5458, 0.5466, 0.5414, 0.5457, 0.5622,
    0.5682, 0.5802, 0.5533, 0.5717, 0.5550, 0.5515, 0.5709, 0.5229, 0.5302, 0.5281,
    0.5334, 0.5052, 0.5241, 0.5326, 0.5382, 0.5229, 0.5350, 0.5475, 0.5377, 0.5376,
    0.5593, 0.5168, 0.5171, 0.5123, 0.5118, 0.4998, 0.4864, 0.5638, 0.4838, 0.4848,
    0.5001, 0.4894, 0.4834, 0.4893, 0.4980, 0.5125, 0.4864, 0.4844, 0.5067, 0.4760
]

epochs = range(1, 51)

plt.figure(figsize=(12, 6))
plt.plot(epochs, monet_gen_loss, label='Monet Generator Loss')
plt.plot(epochs, photo_gen_loss, label='Photo Generator Loss')
plt.plot(epochs, monet_disc_loss, label='Monet Discriminator Loss')
plt.plot(epochs, photo_disc_loss, label='Photo Discriminator Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Generator and Discriminator Losses Over Training')
plt.legend()
plt.show()

"""# **MiFID Score calculations on subset sample (size 1024 )of generated dataset**


"""

monet_ds = get_dataset_jpg_std(MONET_FILENAMES_JPG).batch(1)
photo_ds = get_dataset_jpg_std(PHOTO_FILENAMES_JPG).batch(1)


fast_photo_ds = get_dataset_jpg_std(PHOTO_FILENAMES_JPG).batch(32).prefetch(32)
fid_photo_ds = get_dataset_jpg_std(PHOTO_FILENAMES_JPG).take(1024).batch(32).prefetch(32)
fid_monet_ds = get_dataset_jpg_std(MONET_FILENAMES_JPG).batch(32).prefetch(32)

#  FID score calculation methods
inception_model = tf.keras.applications.InceptionV3(input_shape=(256,256,3),pooling="avg",include_top=False)

mix3  = inception_model.get_layer("mixed9").output
f0 = tf.keras.layers.GlobalAveragePooling2D()(mix3)

inception_model = tf.keras.Model(inputs=inception_model.input, outputs=f0)
inception_model.trainable = False


def calculate_activation_statistics_mod(images,fid_model):

        act=tf.cast(fid_model.predict(images), tf.float32)

        mu = tf.reduce_mean(act, axis=0)
        mean_x = tf.reduce_mean(act, axis=0, keepdims=True)
        mx = tf.matmul(tf.transpose(mean_x), mean_x)
        vx = tf.matmul(tf.transpose(act), act)/tf.cast(tf.shape(act)[0], tf.float32)
        sigma = vx - mx
        return mu, sigma, act

myFID_mu2, myFID_sigma2, myFID_features2 = calculate_activation_statistics_mod(fid_monet_ds, inception_model)
fids=[]

def calculate_memorization_distance(features1, features2):
     neigh = NearestNeighbors(n_neighbors=1, algorithm='kd_tree', metric='euclidean')
     neigh.fit(features2)
     d, _ = neigh.kneighbors(features1, return_distance=True)
     print('d.shape=',d.shape)
     return np.mean(d)

def normalize_rows(x: np.ndarray):
    """
    function that normalizes each row of the matrix x to have unit length.

    Args:
     ``x``: A numpy matrix of shape (n, m)

    Returns:
     ``x``: The normalized (by row) numpy matrix.
    """
    return np.nan_to_num(x/np.linalg.norm(x, ord=2, axis=1, keepdims=True))


def cosine_distance(features1, features2):
    features1_nozero = features1[np.sum(features1, axis=1) != 0]
    features2_nozero = features2[np.sum(features2, axis=1) != 0]
    norm_f1 = normalize_rows(features1_nozero)
    norm_f2 = normalize_rows(features2_nozero)

    d = 1.0-np.abs(np.matmul(norm_f1, norm_f2.T))
    print('d.shape=',d.shape)
    print('np.min(d, axis=1).shape=',np.min(d, axis=1).shape)
    mean_min_d = np.mean(np.min(d, axis=1))
    print('distance=',mean_min_d)
    return mean_min_d


def calculate_frechet_distance(mu1,sigma1,mu2,sigma2):
    fid_epsilon = 1e-14

    covmean = tf.linalg.sqrtm(tf.cast(tf.matmul(sigma1,sigma2),tf.complex64))
#         isgood=tf.cast(tf.math.is_finite(covmean), tf.int32)
#         if tf.size(isgood)!=tf.math.reduce_sum(isgood):
#             return 0

    covmean = tf.cast(tf.math.real(covmean),tf.float32)

    tr_covmean = tf.linalg.trace(covmean)


    return tf.matmul(tf.expand_dims(mu1 - mu2, axis=0),tf.expand_dims(mu1 - mu2, axis=1)) + tf.linalg.trace(sigma1) + tf.linalg.trace(sigma2) - 2 * tr_covmean

def distance_thresholding(d, eps):
    if d < eps:
        return d
    else:
        return 1

COSINE_DISTANCE_EPS = 0.1
def FID(images,gen_model,inception_model=inception_model,myFID_mu2=myFID_mu2, myFID_sigma2=myFID_sigma2, myFID_features2=myFID_features2):
            inp = layers.Input(shape=[256, 256, 3], name='input_image')
            x  = gen_model(inp)
            x=inception_model(x)
            fid_model = tf.keras.Model(inputs=inp, outputs=x)

            mu1, sigma1, features1= calculate_activation_statistics_mod(images,fid_model)

            fid_value = calculate_frechet_distance(mu1, sigma1,myFID_mu2, myFID_sigma2)
            distance = cosine_distance(features1, myFID_features2)

            return fid_value, distance_thresholding(distance, COSINE_DISTANCE_EPS)

# Calculating MiFID score for GAN model
fid_photo_ds = get_dataset_jpg_std(PHOTO_FILENAMES_JPG).take(1024).batch(32).prefetch(32)
fid_value, distance = FID(fid_photo_ds, gan_model_validation.generator)
fid_epsilon = 10e-15

print("FID_public: ", fid_value, "distance_public: ", distance, "multiplied_FID_public: ", fid_value /(distance + fid_epsilon))

# Calculating MiFID score for cycleGAN model
fid_photo_ds = get_dataset_jpg_std(PHOTO_FILENAMES_JPG).take(1024).batch(32).prefetch(32)
fid_value, distance = FID(fid_photo_ds, cyclegan_model.m_gen)
fid_epsilon = 10e-15

print("FID_public: ", fid_value, "distance_public: ", distance, "multiplied_FID_public: ", fid_value /(distance + fid_epsilon))

# Calculating MiFID score for mofified cycleGAN generator with transformer block
fid_photo_ds = get_dataset_jpg_std(PHOTO_FILENAMES_JPG).take(1024).batch(32).prefetch(32)
fid_value, distance = FID(fid_photo_ds, modified_gen_cyclegan_model.m_gen)
fid_epsilon = 10e-15

print("FID_public: ", fid_value, "distance_public: ", distance, "multiplied_FID_public: ", fid_value /(distance + fid_epsilon))

"""# **Visualization of 16 random generated samples**"""

# 16 samples generated by GAN model
monet_example_sample , photo_example_sample = next(iter(val_ds))
for n_sample in range(16):
        input_image = tf.expand_dims(photo_example_sample[n_sample], axis=0)
        generated_sample = gan_model_validation.generator(input_image, training = False)[0]

        f = plt.figure(figsize=(8, 8))

        plt.subplot(121)
        plt.title('Input image')
        plt.imshow(photo_example_sample[n_sample] * 0.5 + 0.5)
        plt.axis('off')

        plt.subplot(122)
        plt.title('Generated image')
        plt.imshow(generated_sample * 0.5 + 0.5)
        plt.axis('off')
        plt.show()

# 16 samples generated by cycleGAN model
monet_example_sample , photo_example_sample = next(iter(val_ds))
for n_sample in range(16):
        input_image = tf.expand_dims(photo_example_sample[n_sample], axis=0)
        generated_sample = cyclegan_model.m_gen(input_image, training = False)[0]

        f = plt.figure(figsize=(8, 8))

        plt.subplot(121)
        plt.title('Input image')
        plt.imshow(photo_example_sample[n_sample] * 0.5 + 0.5)
        plt.axis('off')

        plt.subplot(122)
        plt.title('Generated image')
        plt.imshow(generated_sample * 0.5 + 0.5)
        plt.axis('off')
        plt.show()

# 16 samples generated by modified cycleGAN model
monet_example_sample , photo_example_sample = next(iter(val_ds))
for n_sample in range(16):
        input_image = tf.expand_dims(photo_example_sample[n_sample], axis=0)
        generated_sample = modified_gen_cyclegan_model.m_gen(input_image, training = False)[0]

        f = plt.figure(figsize=(8, 8))

        plt.subplot(121)
        plt.title('Input image')
        plt.imshow(photo_example_sample[n_sample] * 0.5 + 0.5)
        plt.axis('off')

        plt.subplot(122)
        plt.title('Generated image')
        plt.imshow(generated_sample * 0.5 + 0.5)
        plt.axis('off')
        plt.show()

"""# **Conclusion**

Discuss and interpret results as well as learnings and takeaways. What did and did not help improve the performance of your models? What improvements could you try in the future? - (15 pts)

I achieved the best MiFID score of 10.88 with cycleGAN. It is better than vanilla GAN. I implemented modified cycleGAN involving modified generator consisting of transformer blocks, however it achieved MiFID score of 12.58. Also monet images generated by modified cycleGAN has less monet fidelity i.e. less monet style features compared to cycleGAN.

The important takeaway is that training a GAN is an art and requires lot of heuristics, and trials. It's an time intensive process and having a good GPU does help. Also, pre-processing the image training dataset and using the preprocessed training data instead of pre-processsing the dataset during the training does help in speeding up the training process.

There are lot of steps that can be taken to improve the model further. If time permitted, I would have tried:

- Relu units, Gelu units in the activation functions
- 2-headed discriminator blocks and differential augmentation for monet images
- Modifying learning rates using adaptive learning rates during training
- Adding more Drop outs regularization in the convolutional layers
- Transfer Learning: Downloading a trained model, freezing the weights and than training generator and discrimator on the frozen model and further finetuning the complete model.
- Resizing images to higher dimension i.e. 196px x 196 px or 256px x 256 px
- Other architectures i.e. pix2pix, Neural style Transfer etc.

Generating monet images for complete dataset ans submitting to Kaggle
"""

# Loading cycleGAN from saved weights for inference
# Initializing generators and discriminators for CycleGAN architecture
monet_generator = generator_module() # transforms photos to Monet paintings
photo_generator = generator_module() # transforms Monet paintings to be more like photos

monet_discriminator = discriminator_module() # differentiates real Monet paintings and generated Monet paintings
photo_discriminator = discriminator_module() # differentiates real photos and generated photos

# Standard learning rate parameters for CycleGAN generators and discriminators
monet_generator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)
photo_generator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)

monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)
photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-5, beta_1=0.5)

# Create CycleGAN
cyclegan_model = CycleGan(monet_generator, photo_generator,
                      monet_discriminator, photo_discriminator)

cyclegan_model.m_gen = tf.keras.models.load_model("gan-getting-started/models_cyclegan/monet_generator_epoch_35.h5")
cyclegan_model.p_gen = tf.keras.models.load_model("gan-getting-started/models_cyclegan/photo_generator_epoch_35.h5")
cyclegan_model.m_disc = tf.keras.models.load_model("gan-getting-started/models_cyclegan/monet_discriminator_epoch_35.h5")
cyclegan_model.p_disc = tf.keras.models.load_model("gan-getting-started/models_cyclegan/photo_discriminator_epoch_35.h5")


cyclegan_model.compile(m_gen_optimizer=monet_generator_optimizer,
                  p_gen_optimizer=photo_generator_optimizer,
                  m_disc_optimizer=monet_discriminator_optimizer,
                  p_disc_optimizer=photo_discriminator_optimizer,
                  gen_loss_fn=generator_loss,
                  disc_loss_fn=discriminator_loss,
                  cycle_loss_fn=calc_cycle_loss,
                  identity_loss_fn=identity_loss)

photo_ds = get_dataset_jpg_std(PHOTO_FILENAMES_JPG).batch(1)

i = 1
for img in photo_ds:
    prediction = cyclegan_model.m_gen(img, training=False)[0].numpy()
    prediction = (prediction * 127.5 + 127.5).astype(np.uint8)
    im = PIL.Image.fromarray(prediction)
    im.save("gan-getting-started/MiFID_eval/submission/" + str(i) + ".jpg")
    i += 1

shutil.make_archive("gan-getting-started/miFID_eval/submission", 'zip', "gan-getting-started/miFID_eval/submission")
